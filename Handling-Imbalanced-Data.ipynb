{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d40a34",
   "metadata": {},
   "source": [
    "# Phase 2: Handling Imbalanced Data \n",
    "\n",
    "It will split the data, demonstrate how to apply SMOTE, and show how to calculate class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3690f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for this phase\n",
    "import sklearn\n",
    "import imblearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc3f727",
   "metadata": {},
   "source": [
    "### Step 1: Data Splitting\n",
    "\n",
    "First, we separate our features (X) from our target (y) and then split them into training and testing sets. We use stratify=y to ensure that the proportion of defaulted loans is the same in both the train and test splits, which is crucial for imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8824982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Data Splitting ---\n",
      "Data splitting complete.\n",
      "Shape of X_train: (1076248, 93)\n",
      "Shape of X_test: (269062, 93)\n",
      "Shape of y_train: (1076248,)\n",
      "Shape of y_test: (269062,)\n",
      "--------------------------------------------------\n",
      "Original distribution of target variable in the training set:\n",
      "loan_status\n",
      "0    0.800374\n",
      "1    0.199626\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of target variable in the testing set:\n",
      "loan_status\n",
      "0    0.800373\n",
      "1    0.199627\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "The distributions are consistent, thanks to stratification.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This code assumes 'df_final' is the fully preprocessed DataFrame from the end of Phase 2.\n",
    "# Ensure it exists before running this cell.\n",
    "df_final = pd.read_parquet('processed_lending_club_data.parquet')\n",
    "print(\"--- Step 1: Data Splitting ---\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_final.drop('loan_status', axis=1)\n",
    "y = df_final['loan_status']\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "# stratify=y ensures the class distribution is the same in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data splitting complete.\")\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Original distribution of target variable in the training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nDistribution of target variable in the testing set:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "print(\"\\nThe distributions are consistent, thanks to stratification.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4bedb",
   "metadata": {},
   "source": [
    "### Step 2: Implementing SMOTE (Oversampling)\n",
    "\n",
    "Now we apply the SMOTE technique to the training data only. This creates synthetic data points for the minority class (defaulters) to create a balanced dataset for the model to train on. The test set remains untouched and imbalanced, reflecting the real-world data the model will encounter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99386a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE application complete.\n",
      "--------------------------------------------------\n",
      "Class distribution in the original training set (y_train):\n",
      "loan_status\n",
      "0    861401\n",
      "1    214847\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Class distribution after applying SMOTE (y_train_smote):\n",
      "loan_status\n",
      "1    861401\n",
      "0    861401\n",
      "Name: count, dtype: int64\n",
      "\n",
      "The training data is now balanced.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit and apply SMOTE to the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"SMOTE application complete.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Class distribution in the original training set (y_train):\")\n",
    "print(y_train.value_counts())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Class distribution after applying SMOTE (y_train_smote):\")\n",
    "print(y_train_smote.value_counts())\n",
    "print(\"\\nThe training data is now balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cad0b7",
   "metadata": {},
   "source": [
    "### Step 3: Preparing Class Weights\n",
    "\n",
    "As an alternative to oversampling with SMOTE, we can use class weights. This method doesn't change the data itself but adjusts the model's loss function to penalize misclassifying the rare class more heavily. These weights are calculated from the original imbalanced training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1cc5aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights calculation complete.\n",
      "--------------------------------------------------\n",
      "Weight for class 0 (Fully Paid): 0.6247\n",
      "Weight for class 1 (Charged Off): 2.5047\n",
      "\n",
      "These weights tell the model to pay much more attention to class 1 during training.\n",
      "\n",
      "--- Phase 3: Handling Imbalanced Data Complete ---\n",
      "You now have two options for training your models:\n",
      "1. Use the SMOTE-balanced data: (X_train_smote, y_train_smote)\n",
      "2. Use the original imbalanced data (X_train, y_train) with the `class_weight_dict`.\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping class labels to their calculated weights\n",
    "# This is the format required by scikit-learn's `class_weight` parameter\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(\"Class weights calculation complete.\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Weight for class 0 (Fully Paid): {class_weight_dict[0]:.4f}\")\n",
    "print(f\"Weight for class 1 (Charged Off): {class_weight_dict[1]:.4f}\")\n",
    "print(\"\\nThese weights tell the model to pay much more attention to class 1 during training.\")\n",
    "\n",
    "print(\"\\n--- Phase 3: Handling Imbalanced Data Complete ---\")\n",
    "print(\"You now have two options for training your models:\")\n",
    "print(\"1. Use the SMOTE-balanced data: (X_train_smote, y_train_smote)\")\n",
    "print(\"2. Use the original imbalanced data (X_train, y_train) with the `class_weight_dict`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac2ffe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to parquet files...\n",
      "SMOTE-balanced data also saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving data to parquet files...\")\n",
    "\n",
    "# Save feature data (DataFrames)\n",
    "X_train.to_parquet('X_train.parquet')\n",
    "X_test.to_parquet('X_test.parquet')\n",
    "\n",
    "# Save target data (convert Series to DataFrames first)\n",
    "y_train.to_frame().to_parquet('y_train.parquet')\n",
    "y_test.to_frame().to_parquet('y_test.parquet')\n",
    "\n",
    "# Save SMOTE-balanced data if you have it\n",
    "if 'X_train_smote' in locals() and 'y_train_smote' in locals():\n",
    "    X_train_smote.to_parquet('X_train_smote.parquet')\n",
    "    y_train_smote.to_frame().to_parquet('y_train_smote.parquet')\n",
    "    print(\"SMOTE-balanced data also saved.\")\n",
    "\n",
    "with open('class_weight_dict.json', 'w') as f:\n",
    "    json.dump(class_weight_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit_default_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
