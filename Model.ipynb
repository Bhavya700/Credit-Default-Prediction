{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b439d4c6",
   "metadata": {},
   "source": [
    "# Phase 3: Model Building \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d589aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Import the models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3cb15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data splits...\n",
      "Loading SMOTE-balanced training data...\n",
      "Loading class weight dictionary...\n",
      "All data loaded successfully.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data splits...\")\n",
    "X_train = pd.read_parquet('X_train.parquet')\n",
    "y_train = pd.read_parquet('y_train.parquet').squeeze() # .squeeze() to convert from DataFrame to Series\n",
    "X_test = pd.read_parquet('X_test.parquet')\n",
    "y_test = pd.read_parquet('y_test.parquet').squeeze()\n",
    "\n",
    "print(\"Loading SMOTE-balanced training data...\")\n",
    "X_train_smote = pd.read_parquet('X_train_smote.parquet')\n",
    "y_train_smote = pd.read_parquet('y_train_smote.parquet').squeeze()\n",
    "\n",
    "print(\"Loading class weight dictionary...\")\n",
    "with open('class_weight_dict.json', 'r') as f:\n",
    "    class_weight_dict = json.load(f)\n",
    "    # JSON saves keys as strings, so we convert the '0' and '1' keys back to integers\n",
    "    class_weight_dict = {int(k): v for k, v in class_weight_dict.items()}\n",
    "\n",
    "print(\"All data loaded successfully.\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382916f2",
   "metadata": {},
   "source": [
    "### Step 1: Baseline Model - Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8b9a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Logistic Regression Models ---\n",
      "Training Logistic Regression with class weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavyapatel/Documents/CS/Credit Default Prediction/credit_default_env/lib/python3.13/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Training Logistic Regression with SMOTE data...\n",
      "Done.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavyapatel/Documents/CS/Credit Default Prediction/credit_default_env/lib/python3.13/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trained_models = {}\n",
    "print(\"--- Training Logistic Regression Models ---\")\n",
    "\n",
    "# Model 1.1: Logistic Regression with Class Weights\n",
    "print(\"Training Logistic Regression with class weights...\")\n",
    "lr_weighted = LogisticRegression(solver='liblinear', class_weight=class_weight_dict, random_state=42)\n",
    "lr_weighted.fit(X_train, y_train)\n",
    "trained_models['Logistic Regression (Weighted)'] = lr_weighted\n",
    "print(\"Done.\")\n",
    "\n",
    "# Model 1.2: Logistic Regression with SMOTE Data\n",
    "print(\"Training Logistic Regression with SMOTE data...\")\n",
    "lr_smote = LogisticRegression(solver='liblinear', random_state=42)\n",
    "lr_smote.fit(X_train_smote, y_train_smote)\n",
    "trained_models['Logistic Regression (SMOTE)'] = lr_smote\n",
    "print(\"Done.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527d4c1",
   "metadata": {},
   "source": [
    "### Step 2: Ensemble Model - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5deda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training Random Forest Models ---\")\n",
    "\n",
    "# Model 2.1: Random Forest with Class Weights\n",
    "# Using n_jobs=-1 to use all available CPU cores for faster training\n",
    "print(\"Training Random Forest with class weights...\")\n",
    "rf_weighted = RandomForestClassifier(n_estimators=100, class_weight=class_weight_dict, random_state=42, n_jobs=-1)\n",
    "rf_weighted.fit(X_train, y_train)\n",
    "trained_models['Random Forest (Weighted)'] = rf_weighted\n",
    "print(\"Done.\")\n",
    "\n",
    "# Model 2.2: Random Forest with SMOTE Data\n",
    "print(\"Training Random Forest with SMOTE data...\")\n",
    "rf_smote = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "trained_models['Random Forest (SMOTE)'] = rf_smote\n",
    "print(\"Done.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72eca91",
   "metadata": {},
   "source": [
    "### Step 3: Advanced Ensemble Model - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training LightGBM Models ---\")\n",
    "\n",
    "# Model 3.1: LightGBM with Class Weights\n",
    "# LightGBM uses `class_weight` in the same way as scikit-learn\n",
    "print(\"Training LightGBM with class weights...\")\n",
    "lgbm_weighted = LGBMClassifier(class_weight=class_weight_dict, random_state=42)\n",
    "lgbm_weighted.fit(X_train, y_train)\n",
    "trained_models['LightGBM (Weighted)'] = lgbm_weighted\n",
    "print(\"Done.\")\n",
    "\n",
    "# Model 3.2: LightGBM with SMOTE Data\n",
    "# Note: LightGBM also has a built-in `is_unbalance=True` parameter which is another alternative.\n",
    "# Here, we will stick to the specified plan of using the SMOTE-generated data.\n",
    "print(\"Training LightGBM with SMOTE data...\")\n",
    "lgbm_smote = LGBMClassifier(random_state=42)\n",
    "lgbm_smote.fit(X_train_smote, y_train_smote)\n",
    "trained_models['LightGBM (SMOTE)'] = lgbm_smote\n",
    "print(\"Done.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d0d70",
   "metadata": {},
   "source": [
    "### Step 4: Save Trained Models to File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81404c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'trained_models.joblib'\n",
    "joblib.dump(trained_models, model_filename)\n",
    "\n",
    "print(f\"All models have been trained and saved to '{model_filename}'\")\n",
    "print(\"\\n--- Phase 4: Model Building Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit_default_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
